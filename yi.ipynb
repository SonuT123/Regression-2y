{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc8258a-4dfa-4b8a-a1b1-c45b1f5196e3",
   "metadata": {},
   "source": [
    "                                               Regression-2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8915a329-b340-4706-af0f-4f5c768446c0",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f5c191-65e5-4fb1-8805-ef10717915a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "1.Definition:\n",
    "    R-squared, also known as the coefficient of determination, is a goodness-of-fit measure for linear regression models.\n",
    "    It quantifies the strength of the relationship between the independent variables (predictors) and the dependent variable (response)\n",
    "    in a regression model.\n",
    "    R-squared represents the percentage of the variance in the dependent variable that the independent variables collectively explain.\n",
    "    \n",
    "2.Calculation:\n",
    "    R-squared is always between 0% and 100%.\n",
    "    Here’s how it’s calculated:\n",
    "    Start by fitting a linear regression model to your data.\n",
    "    Compute the sum of squared residuals (the differences between observed values and predicted values).\n",
    "    Next, calculate the total sum of squares (TSS), which represents the total variation in the dependent variable around its mean.\n",
    "    Finally, compute R-squared using the \n",
    "    formula: [ R^2 = 1 - \\frac{{\\text{{Sum of squared residuals}}}}{{\\text{{Total sum of squares}}}} ]\n",
    "    \n",
    "3.Interpretation:\n",
    "    An R-squared value of 0% indicates that the model does not explain any variation in the response variable beyond its mean.\n",
    "    An R-squared value of 100% means that the model explains all the variation in the response variable.\n",
    "    However, be cautious:\n",
    "    Small R-squared values are not necessarily problematic. Sometimes, even simple models can be useful.\n",
    "    High R-squared values do not always imply a good model. Overfitting can lead to artificially high R-squared values."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2986e6a-c150-42ec-9da7-ee617dc57ada",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d19070b-ac64-4e90-86e3-0e22933fd399",
   "metadata": {},
   "source": [
    "1.Definition:\n",
    "Adjusted R-squared is an enhanced version of the regular R-squared (coefficient of determination) used in linear regression models.\n",
    "  It addresses a limitation of the regular R-squared by penalizing the inclusion of unnecessary predictors in the model.\n",
    "\n",
    "2.Calculation:\n",
    "    The formula for adjusted R-squared \n",
    "    is: [ \\text{{Adjusted R-squared}} = 1 - \\frac{{\\frac{{\\text{{Sum of squared residuals}}}}{{\\text{{Degrees\n",
    "    of freedom (n - k - 1)}}}}}}{{\\frac{{\\text{{Total sum of squares}}}}{{\\text{{Degrees of freedom (n - 1)}}}}}} ]\n",
    "    (n) represents the number of observations (data points).\n",
    "    (k) represents the number of predictors (independent variables) in the model.\n",
    "    Unlike regular R-squared, which always increases when more predictors are added (even if they are irrelevant),\n",
    "    adjusted R-squared accounts for model complexity.\n",
    "    \n",
    "3.Differences:\n",
    "   Regular R-squared:\n",
    "      Measures the proportion of variance explained by all predictors (both relevant and irrelevant).\n",
    "      Increases as more predictors are added, even if they don’t improve the model significantly.\n",
    "      Can be misleading when adding unnecessary predictors.\n",
    "        \n",
    "   Adjusted R-squared:\n",
    "      Penalizes the inclusion of unnecessary predictors.\n",
    "      Reflects the trade-off between model fit and complexity.\n",
    "      Decreases if adding a predictor doesn’t improve the model significantly.\n",
    "      Provides a more realistic assessment of model performance.\n",
    "    \n",
    "4.Interpretation:\n",
    "  A higher adjusted R-squared indicates that the model explains more variation in the response variable\n",
    "  while accounting for model complexity.\n",
    "   Researchers often prefer adjusted R-squared when comparing models with different numbers of predictors."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e6072c0-5718-4242-9180-96c78bad0363",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f77925a-8cb6-4052-98e7-e1464f930fe9",
   "metadata": {},
   "source": [
    "Adjusted R-squared is particularly useful in scenarios where you want to balance model fit with model complexity\n",
    "\n",
    "1.Comparing Models:\n",
    "When comparing multiple regression models with different numbers of predictors, adjusted R-squared provides a \n",
    "    better assessment of their performance.\n",
    "    Regular R-squared tends to increase as more predictors are added, even if they are irrelevant.\n",
    "    Adjusted R-squared penalizes unnecessary predictors, making it a fairer comparison metric.\n",
    "2.Model Selection:\n",
    "  During variable selection (choosing which predictors to include in the model), adjusted R-squared helps identify relevant predictors.\n",
    "  It discourages adding predictors that don’t significantly improve model fit.\n",
    "3.Avoiding Overfitting:\n",
    "   Overfitting occurs when a model captures noise or random fluctuations in the data.\n",
    "   Adjusted R-squared helps prevent overfitting by considering model complexity.\n",
    "   A high regular R-squared may indicate overfitting, but a lower adjusted R-squared suggests a more balanced model.\n",
    "4.Interpreting Model Fit:\n",
    "   While regular R-squared focuses solely on explained variance, adjusted R-squared accounts for degrees\n",
    "   of freedom (number of predictors).\n",
    "   Adjusted R-squared provides a more realistic view of how well the model generalizes to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa0970-3721-4ac2-9c54-15536b380c9b",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03216a29-e8f4-4838-9b7d-9132191882d1",
   "metadata": {},
   "source": [
    "Regression analysis,  RMSE (Root Mean Squared Error), MSE (Mean Squared Error) and MAE (Mean Absolute Error) are\n",
    "all metrics used to evaluate how well a regression model fits the data. They all measure the difference between\n",
    "the actual values (y-axis) and the predicted values by the model. Lower values of these metrics indicate a better fit.\n",
    "\n",
    "breakdown of each metric:\n",
    "\n",
    "   Mean Squared Error (MSE):\n",
    "   Calculates the average squared difference between the actual and predicted values. \n",
    "   Squaring the differences gives more weight to larger errors.\n",
    "   MSE is calculated by:\n",
    "   Taking the difference between each actual value (y_i) and its corresponding predicted value (ŷ_i).\n",
    "   Squaring these differences.\n",
    "   Finding the average of the squared differences.\n",
    "\n",
    "  Root Mean Squared Error (RMSE):\n",
    "   This is derived from MSE. \n",
    "   It takes the square root of the MSE.\n",
    "   RMSE has the same units as the original variable, making it easier to interpret the magnitude of the error.\n",
    "   While MSE is used for mathematical calculations due to its differentiability, RMSE is more commonly used to\n",
    "   report the error because it's easier to understand.\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "   Calculates the average of the absolute differences between the actual and predicted values.\n",
    "   Unlike MSE, it doesn't square the errors, so all errors are weighted equally.\n",
    "   MAE is calculated by:\n",
    "   Taking the absolute difference between each actual value (y_i) and its corresponding predicted value (ŷ_i).\n",
    "   Finding the average of these absolute differences. \n",
    "\n",
    "Choosing the right metric:\n",
    "\n",
    " MSE and RMSE are more sensitive to outliers than MAE, since squaring the errors magnifies their impact.\n",
    " If the presence of outliers is a concern, MAE might be a better choice.\n",
    " When the errors are normally distributed and the scale of the prediction is important, RMSE is preferred.\n",
    " Ultimately, the best metric depends on the specific context of your regression problem."
   ]
  },
  {
   "cell_type": "raw",
   "id": "57080bea-53a9-463f-afda-7ff91f4623b7",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7071715-f2f9-44b5-8e9e-8dc04dd677da",
   "metadata": {},
   "source": [
    "Mean Absolute Error (MAE):\n",
    "1.Advantages:\n",
    "Robustness to Outliers: MAE is less sensitive to outliers because it considers absolute errors.\n",
    "           It treats positive and negative deviations equally.\n",
    "           Easy Interpretation: MAE represents the average magnitude of prediction errors in the original unit of measurement,\n",
    "           making it straightforward to understand.\n",
    "    Differentiable: MAE is differentiable, which is useful for optimization algorithms.\n",
    "    \n",
    "Disadvantages:\n",
    "  Lack of Sensitivity: Since MAE treats all errors equally, it may not penalize larger errors enough.\n",
    "2.Mean Squared Error (MSE):\n",
    "Advantages:\n",
    "   Mathematical Properties: MSE is differentiable, convex, and widely used in optimization algorithms.\n",
    "    Emphasis on Larger Errors: Squaring the errors in MSE gives more weight to larger deviations, which can be beneficial.\n",
    "    \n",
    "Disadvantages:\n",
    "    Sensitivity to Outliers: MSE is sensitive to outliers due to the squaring operation.\n",
    "    Units: MSE is not in the original unit of measurement, making interpretation challenging.\n",
    "    Root Mean Squared Error (RMSE):\n",
    "Advantages:\n",
    "     Interpretability: RMSE returns the error metric to the same unit as the target variable, making it easier to interpret.\n",
    "     Similar to MSE: RMSE inherits the benefits of MSE.\n",
    "Disadvantages:\n",
    "    Outlier Sensitivity: Like MSE, RMSE is sensitive to outliers.\n",
    "    Complexity: RMSE involves taking the square root, which can be computationally expensive.\n",
    "3.Choosing the Right Metric:\n",
    "    Context Matters: Select the metric based on the problem context. For example:\n",
    "    Use MAE when you want to focus on the average magnitude of errors without emphasizing outliers.\n",
    "    Use MSE or RMSE when you want to penalize larger errors more heavily.\n",
    "    Trade-offs: Consider the trade-offs between robustness, interpretability, and sensitivity to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f588f1-5eb5-4203-8d4f-799383511332",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df17919-09c8-4381-8b99-a071d8408952",
   "metadata": {},
   "source": [
    "Lasso regularization, also known as L1 regularization, is a technique used to prevent overfitting in linear regression models.\n",
    "  It achieves this by adding a penalty term to the loss function during model training.\n",
    "  This penalty term encourages the model to shrink the coefficients of some features towards zero,\n",
    "  effectively performing feature selection.\n",
    "\n",
    " breakdown of Lasso and how it compares to Ridge regularization:\n",
    "\n",
    "Lasso Regularization:\n",
    "\n",
    " Penalty Term: The penalty term in Lasso is the sum of the absolute values of all the coefficients (L1 norm).\n",
    "  Coefficient Shrinkage: As the value of the regularization parameter (lambda) increases, more coefficients are driven to zero.\n",
    "    This leads to feature selection, as features with coefficients of zero are no longer considered by the model.\n",
    "    \n",
    " Strengths:\n",
    " Performs feature selection, which can improve model interpretability and reduce overfitting.\n",
    " Can be useful when there are correlated features, as it tends to select only one from a group of highly correlated features.\n",
    "    \n",
    "Ridge Regularization (L2 Regularization):\n",
    "\n",
    " Penalty Term: The penalty term in Ridge regression is the sum of the squared values of all the coefficients (L2 norm).\n",
    "  Coefficient Shrinkage: Ridge regression shrinks all coefficients towards zero but never sets them to zero.\n",
    "  This reduces the magnitude of coefficients but retains all features in the model.\n",
    "\n",
    "Strengths:\n",
    "  Improves model stability and reduces overfitting by shrinking coefficients.\n",
    "  Can be beneficial when dealing with collinear features, as it reduces the impact of their correlation on the model.\n",
    "  When to Use Lasso:\n",
    "\n",
    "  Feature Selection: If your primary goal is to identify the most important features for prediction, Lasso is a better\n",
    "  choice due to its ability to drive coefficients to zero.\n",
    "    \n",
    "  High-Dimensional Data: When you have a large number of features compared to the number of data points, Lasso can help \n",
    "  reduce model complexity and improvegeneralizability.\n",
    "    \n",
    "  Interpretability: Lasso simplifies the model by selecting a smaller subset of features, making it easier to interpret \n",
    "  the relationships between features and the target variable.\n",
    "   When to Use Ridge:\n",
    "\n",
    "   Overfitting Prevention: If your main concern is overfitting and interpretability is less important, Ridge regression can be\n",
    "   a good choice for reducing coefficient magnitudes and improving model stability.\n",
    "   Collinear Features: When you suspect your data has correlated features, Ridge regression can help mitigate their impact on the\n",
    "   model by shrinking coefficients but retaining all features.\n",
    "In conclusion:\n",
    "\n",
    "Both Lasso and Ridge regularization are valuable tools for combating overfitting in linear regression.\n",
    "   The choice between them depends on your specific needs. If feature selection and interpretability are priorities, Lasso shines.\n",
    "   If overfitting prevention and handling correlated features are your main concerns, Ridge might be a better fit.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1cfd3ad1-eaba-4783-9123-864ce1091022",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e33a9f5-d7cf-4935-b6c9-e0be4fd9f324",
   "metadata": {},
   "source": [
    "1. Overfitting:\n",
    "\n",
    "    Overfitting occurs when a model learns the training data too well, capturing noise and random fluctuations rather\n",
    "       than the underlying patterns.\n",
    "    An overfit model performs exceptionally well on the training data but poorly on unseen data (test/validation data).\n",
    "    \n",
    "2. Regularization Techniques:\n",
    "\n",
    "   Regularization introduces a penalty term to the cost function during model training.\n",
    "   It discourages the model from becoming too complex by controlling the magnitude of coefficients (weights assigned to features).\n",
    "    \n",
    "3. Lasso and Ridge as Examples:\n",
    "\n",
    "   Let’s consider a housing price prediction example:\n",
    "   Dataset: We have data on house features (square footage, number of bedrooms, etc.) and their corresponding prices.\n",
    "   Goal: Build a regression model to predict house prices.\n",
    "    \n",
    "Unregularized Model (Overfitting):\n",
    "    Without regularization, the model might fit every detail in the training data, including noise.\n",
    "    Example: The model assigns high importance to a specific realtor in the training data,\n",
    "    even though that’s not generally a significant factor in house prices.\n",
    "   Result: Overfitting, poor generalization to unseen data.\n",
    "   Regularized Models (Lasso and Ridge):\n",
    "   Both Lasso and Ridge add a penalty term to the cost function.\n",
    "  They balance fitting the data well with keeping model coefficients small.\n",
    " Lasso:\n",
    "  Objective: Encourages sparsity by shrinking some coefficients to exactly zero.\n",
    "  Effect: Removes less relevant features (automatic feature selection).\n",
    "  Use Case: When dealing with high-dimensional data and suspecting irrelevant features.\n",
    " Ridge:\n",
    "  Objective: Reduces coefficient magnitudes without forcing them to zero.\n",
    "  Effect: Stabilizes coefficient estimates, especially when predictors are highly correlated (multicollinearity).\n",
    "  Use Case: When multicollinearity is a concern.\n",
    "\n",
    "4. Illustrative Example:\n",
    "\n",
    "  Imagine our dataset contains features like square footage, number of bedrooms, and a “realtor score.”\n",
    "  Unregularized Model:\n",
    "  Assigns high weight to the “realtor score,” even if it’s not relevant.\n",
    "  Overfits to noise.\n",
    "  Regularized Models:\n",
    "  Lasso: Shrinks the “realtor score” coefficient towards zero or eliminates it entirely.\n",
    "  Ridge: Reduces the impact of the “realtor score” without excluding it.\n",
    "  Result: More robust, generalizable models."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d3ea210-cc1a-4b4c-8d9f-28c124687fdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f7000a-37f9-4012-8ff5-5ba83f55256a",
   "metadata": {},
   "source": [
    "1.Loss of Interpretability:\n",
    "Regularization techniques like Lasso and Ridge alter the coefficients of features.\n",
    "While this improves generalization, it can make the model less interpretable.\n",
    "In some cases, understanding the impact of individual features is crucial (e.g., in medical or legal contexts).\n",
    "\n",
    "2.Feature Selection Bias:\n",
    "Lasso, by design, encourages sparsity by setting some coefficients to zero.\n",
    "However, this automatic feature selection may exclude relevant features.\n",
    "If domain knowledge suggests all features are important, Lasso might not be ideal.\n",
    "\n",
    "3.Hyperparameter Tuning:\n",
    "Regularization introduces hyperparameters (e.g., (\\lambda) in Lasso and Ridge).\n",
    "Choosing the right value requires cross-validation and tuning.\n",
    "Incorrect hyperparameter selection can lead to suboptimal results.\n",
    "\n",
    "4.Multicollinearity Handling:\n",
    "Ridge helps with multicollinearity, but it doesn’t eliminate it entirely.\n",
    "If predictors are highly correlated, other techniques (e.g., PCA) might be more effective.\n",
    "\n",
    "5.Assumption Violation:\n",
    "Regularized linear models assume linearity between predictors and response.\n",
    "If the relationship is nonlinear, other models (e.g., decision trees, neural networks) may perform better.\n",
    "\n",
    "6.Outliers and Sensitivity:\n",
    "Regularization is sensitive to outliers.\n",
    "Extreme outliers can disproportionately affect the penalty term.\n",
    "Robust regression methods might be more suitable in such cases.\n",
    "\n",
    "7.Data Scaling Dependency:\n",
    "Regularization depends on feature scaling.\n",
    "If features have different scales, the penalty term may unfairly impact certain coefficients.\n",
    "Standardizing features is essential.\n",
    "\n",
    "8.Alternative Models:\n",
    "Regularized linear models are just one approach.\n",
    "Depending on the problem, other models (e.g., SVMs, random forests) might yield better results."
   ]
  },
  {
   "cell_type": "raw",
   "id": "db759840-66f0-41dc-9f8a-167e5e93d463",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "601bb024-cf3c-4a96-879b-448c488e6336",
   "metadata": {},
   "source": [
    "1.Model A (RMSE = 10):\n",
    "Root Mean Squared Error (RMSE) measures the average magnitude of prediction errors, considering both small and large errors.\n",
    "    RMSE is sensitive to outliers because it squares the errors.\n",
    "    A lower RMSE indicates better performance, as it means smaller prediction errors.\n",
    "    \n",
    "2.Model B (MAE = 8):\n",
    "    Mean Absolute Error (MAE) also measures the average magnitude of prediction errors but treats positive and negative errors equally.\n",
    "    MAE is less sensitive to outliers because it uses absolute values.\n",
    "    A lower MAE is preferable, indicating smaller average errors.\n",
    "    \n",
    "Comparison:\n",
    "\n",
    "  Model A has a higher RMSE (larger errors) but might be more sensitive to outliers.\n",
    "  Model B has a lower MAE (smaller errors) and is robust to outliers.\n",
    "  Choosing the Better Model:\n",
    "\n",
    "Given the metrics:\n",
    "   RMSE: 10\n",
    "   MAE: 8\n",
    "Model B (MAE = 8) is the better performer because it has smaller average errors.\n",
    " Limitations of the Metric Choice:\n",
    "\n",
    " While MAE is a good choice, it has limitations:\n",
    " It treats all errors equally, regardless of their magnitude.\n",
    " It doesn’t penalize larger errors more heavily (unlike RMSE).\n",
    "  Depending on the specific problem, you might prioritize different aspects (e.g., small errors vs. sensitivity to outliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd81b9-52d5-4863-969a-b8450ca13c58",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da1bba5f-7bd1-4fec-998b-356b857f93fd",
   "metadata": {},
   "source": [
    "1.Model A (Ridge Regularization):\n",
    "Objective: Ridge regularization (L2 regularization) adds a penalty term based on the squared coefficients (slopes).\n",
    "Effect:\n",
    "     Ridge coefficients can approach zero but will not be exactly zero.\n",
    "     It reduces the magnitude of coefficients without forcing them to zero.\n",
    "     Ridge helps stabilize coefficient estimates and prevents overfitting.\n",
    "Trade-offs:\n",
    "      Ridge increases bias by shrinking coefficients but reduces variance.\n",
    "      It strikes a balance between fitting the training data well (low bias) and maintaining\n",
    "      good generalization to unseen data (low variance).\n",
    "        \n",
    "2.Model B (Lasso Regularization):\n",
    "    Objective: Lasso regularization (L1 regularization) adds a penalty term based on the absolute values of coefficients.\n",
    "\n",
    "    Effect:\n",
    "    Lasso coefficients can become exactly zero, effectively removing corresponding features.\n",
    "    It performs automatic feature selection by excluding less relevant features.\n",
    "    \n",
    "Trade-offs:\n",
    "     Lasso reduces model complexity by sparsity (some coefficients are exactly zero).\n",
    "     It can lead to feature selection bias if relevant features are excluded.\n",
    "     Lasso is suitable when dealing with highly correlated features and feature reduction is desired.\n",
    "Choosing the Better Model:\n",
    "\n",
    "  Both Ridge and Lasso have their advantages and limitations.\n",
    "  The choice depends on the problem context and priorities:\n",
    "  If you suspect irrelevant features and want feature selection, Lasso (Model B) is preferable.\n",
    "  If multicollinearity is a concern and you want stable coefficient estimates, Ridge (Model A) is a better choice.\n",
    " Limitations and Considerations:\n",
    "\n",
    "  Bias-Variance Trade-off: Both regularization methods aim to strike a balance between bias and variance.\n",
    "  Hyperparameter Tuning: Proper tuning of the regularization parameter ((\\lambda)) is crucial for optimal performance.\n",
    "  Interpretability: Regularized models may be less interpretable due to modified coefficients.\n",
    "  Context Matters: Consider the specific problem, data, and trade-offs when choosing regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab27a8-de8e-4438-8221-86d93b3245fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
